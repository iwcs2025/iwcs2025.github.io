Keynotes
====

<br>

Oana-Maria Camburu (Imperial College London)
----
<img style="display: block;margin: 25px;max-width: 30%;" src="photos/OMCamburu.png" align="left">

Oana-Maria Camburu is an Assistant Professor in the Department of Computing at Imperial College London. Prior to that, she was a Principal Research Fellow in the Department of Computer Science at the University College London, holding an Early Career Leverhulme Fellowship. Oana was also a postdoc at the University of Oxford, from where she obtained her PhD in "Explaining Deep Neural Networks". Her main research interests lie in explainability for deep learning models and AI safety and alignment. 

<br>
<br>
<br>

**Title**: _Thoughts You Can Trust? Evaluating the Faithfulness of  Model-Generated Explanations and Their Effects on Human Performance_

**Abstract**: Large Language Models (LLMs) can readily generate natural language explanations—or chain-of-thoughts (CoTs)—to justify their outputs. In this talk, I will first introduce methods for evaluating whether such explanations faithfully reflect the decision-making processes of the models that produce them. Second, I present the results of a user study involving 85 clinicians and medical students diagnosing chest X-rays. The study compares the effectiveness of natural language explanations, saliency maps, and their combination in supporting clinical decision-making.


<br>

Alexander Koller (Saarland University)
----

<div>
<img style="display: block;margin: 25px;max-width: 30%;" src="photos/koller-small.jpeg" align="left">
<br>

Alexander Koller is a Professor of Computational Linguistics at
Saarland University in Saarbrücken, Germany. His research interests
include planning and reasoning with LLMs, syntactic and semantic
processing, natural language generation, and dialogue systems. He is
particularly interested in neurosymbolic models that bring together
principled linguistic modeling and correctness guarantees with the
coverage and robustness of neural approaches. Alexander received his
PhD from Saarland University and was previously a postdoc at Columbia
University and the University of Edinburgh, faculty at the University
of Potsdam, and Visiting Senior Research Scientist at the Allen
Institute for AI.
</div>

<br>
<br>
<br>

**Title**: _Solving Complex Problems with Large Language Models_

**Abstract**: One of the great promises that people connect with LLMs is that they can make complex problem-solving with computers accessible to lay users. Unlike traditional symbolic solvers (e.g. for planning or constraint solving), LLMs accept natural-language input and require no expert training; unlike earlier task-oriented dialogue systems, they can be applied across arbitrary domains. In my talk, I will explore the degree to which LLMs are fulfilling this promise. I will present recent work on whether current LLMs "reason" or "recite", discuss the role of symbolic representations in LLM-based problem-solving, and offer some thoughts on trustworthy problem-solving with LLMs.

<br>

Denis Paperno (Utrecht University)
---

<div>
<img style="display: block;margin: 25px;max-width: 30%;" src="photos/denis_on_green.jpg" align="left">
<br>

Denis Paperno is assistant professor of computational linguistics at Utrecht University. He received a PhD in Linguistics from the University of California Los Angeles, and subsequently worked at the University of Trento (CLIC lab, Rovereto) as a postdoc and at the Loria lab (Nancy) as a CNRS researcher. Denis has published extensively in the fields of semantics, language model evaluation, and vector space representations of meaning. His research contributions include work on compositionality in computational models of semantics, visual grounding, and representation probing.
</div>

<br>
<br>
<br>

**Title**: _Compositionality, Intensionality and LLMs: The Case of the Personal Relations Task_

**Abstract**: Semanticists have long considered compositionality to be at the heart of natural language interpretation. Modern large language models (LLMs) achieve impressive results at tasks involving semantics, but in most cases it is hard to judge to what extent they rely on compositional mechanisms. Since the training data is enormous and could contain many complex phrases, much of LLM's performance could potentially be attributed to non-compositional pattern memorization, leaving little space for compositional ability. For example, "mother of Elon Musk" could be processed by a language model as a holistic unit since the phrase occurs in this form in the training corpora. The talk will discuss ongoing work based on the Personal Relations task (Paperno, 2022), designed to assess semantic compositionality properly. The Personal Relations task relies on a small universe with randomly generated relations which can not be present in language model pretraining, therefore offering a testing ground for compositional abilities of models at phrase level. Furthermore, the Personal Relations task allows us to contrast intensional and extensional semantic interpretation. We find that language models (still) exhibit different compositional abilities than humans, with intensionality playing a substantial role.

<br>
<br>
<br>



